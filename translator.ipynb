{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "tensorflow.random.set_seed(12)\n",
    "keras.utils.set_random_seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"spanish-english.txt\"\n",
    "\n",
    "# import the data\n",
    "data = open(data_path, 'r', encoding='utf-8') \n",
    "# save each line from the txt file as an item in a list\n",
    "lines = data.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup docs\n",
    "Docs refer to the whole input / output of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists to hold sentences\n",
    "input_docs = []\n",
    "target_docs = []\n",
    "\n",
    "for line in lines[:100000]:\n",
    "    # input and target sentences are separated by tabs\n",
    "    input_doc, target_doc = line.split('\\t')[:2]\n",
    "\n",
    "    # add each input sentence to the docs list\n",
    "    input_docs.append(input_doc)\n",
    "\n",
    "    # separate each token in the input doc by a space\n",
    "    # e.g. 'Ve.' becomes 'Ve .'\n",
    "    target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
    "    # append start and end tokens to the beginning and end of doc\n",
    "    target_doc = '<START> ' + target_doc + ' <END>'\n",
    "    target_docs.append(target_doc)\n",
    "\n",
    "print(\"Example input sentence:\", input_docs[9])\n",
    "print(\"Example target sentence:\", target_docs[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup tokens\n",
    "Tokens are the vocabulary sets of LLMs. Each token is a word the LLMs anticipates it'll receive or output.\n",
    "\n",
    "Max encoder seq lengths will tell us the max number of tokens observed in input and target docs. This will inform the max output length we want the translator to generate when making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty sets to hold tokens\n",
    "input_tokens = []\n",
    "target_tokens = []\n",
    "\n",
    "# max number of tokens in a doc\n",
    "max_encoder_seq_length = 0\n",
    "max_decoder_seq_length = 0\n",
    "\n",
    "for input_doc in input_docs:\n",
    "    tokens = re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)\n",
    "\n",
    "    # if number of tokens is higher than the previous\n",
    "    # highest max_encoder_seq_length value then update it\n",
    "    if len(tokens) > max_encoder_seq_length:\n",
    "         max_encoder_seq_length = len(tokens)\n",
    "\n",
    "    # add each word or punctuation from the input sentences\n",
    "    # to the tokens list if it's not already there\n",
    "    for token in tokens:\n",
    "        if token not in input_tokens:\n",
    "                input_tokens.append(token)\n",
    "\n",
    "# repeat for the target set\n",
    "for target_doc in target_docs:\n",
    "    tokens = target_doc.split()\n",
    "    if len(tokens) > max_decoder_seq_length:\n",
    "         max_decoder_seq_length = len(tokens)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in target_tokens:\n",
    "            target_tokens.append(token)\n",
    "\n",
    "# take away two from the decoder token length as we don't want to\n",
    "# include <START> and <END> tokens we added earlier in our outputs\n",
    "# max_decoder_seq_length -=2\n",
    "\n",
    "# alphabeticaly sort the tokens\n",
    "input_tokens = sorted(input_tokens)\n",
    "target_tokens = sorted(target_tokens)\n",
    "\n",
    "print(\"Example input word:\", input_tokens[2])\n",
    "print(\"Example target word:\", target_tokens[2])\n",
    "print(\"Max encoder seq length:\", max_encoder_seq_length)\n",
    "print(\"Max decoder seq length:\", max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features_dict = {}\n",
    "target_features_dict = {}\n",
    "reverse_input_features_dict = {}\n",
    "reverse_target_features_dict = {}\n",
    "\n",
    "for i, token in enumerate(input_tokens):\n",
    "    input_features_dict[token] = i\n",
    "    reverse_input_features_dict[i] = token\n",
    "\n",
    "for i, token in enumerate(target_tokens):\n",
    "    target_features_dict[token] = i\n",
    "    reverse_target_features_dict[i] = token\n",
    "\n",
    "print(input_features_dict)\n",
    "print(reverse_input_features_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a three three dimensional arrays of zeros: encoder_input_data, decoder_input_data and decoder_target_data.\n",
    "\n",
    "The first dimension represents the number of docs (sentences) we have.\n",
    "The second dimension represents the timestep, which has the same max value as the highest number of tokens (words) found across all of the samples.\n",
    "The last dimension represents the number of individual tokens available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode data\n",
    "No we one-hot encode the data so that our model can process it.\n",
    "We iterate over each document. In that document, for every token position, we set the the value of the present token to 1.\n",
    "So in this sentence: `We go shopping` at the first index, the token 'We' would get assigned a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip the input and target docs together so that each an input and target of the\n",
    "# corresponding index are collated together on one line\n",
    "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
    "\n",
    "    for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n",
    "        encoder_input_data[line, timestep, input_features_dict[token]] = 1.\n",
    "\n",
    "    for timestep, token in enumerate(target_doc.split()):\n",
    "        decoder_input_data[line, timestep, target_features_dict[token]] = 1.\n",
    "\n",
    "        if timestep > 0:\n",
    "            decoder_target_data[line, timestep - 1, target_features_dict[token]] = 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "batch_size = 96\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the shape of the encoder inputs\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "# define the encoder LSTM layer\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "# connect the encoder inputs to the LSTM layer\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the shape of encoder inputs\n",
    "When we define the shape of the encoder inputs, since the sentences can be of varying length the first param is 'None'. `num_encoder_tokens` represents the number of unique tokens.\n",
    "\n",
    "### Defining the LSTM layer\n",
    "latent_dim sets the dimensionality of the network. Usually, the higher this is the better the network can capture complex patterns but it then also requires more resources.\n",
    "\n",
    "Setting the `return_state` to True ensures the LSTM returns its hidden and cell states.\n",
    "\n",
    "### Connecting inputs to the LSTM layer\n",
    "From this we extract `encoder_outputs` - the output of the layer for each timestep\n",
    "`state_hidden` and `state_cell` hold the final hidden and cell states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# setup the dense layer for predictions\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "# store the final output in `decoder_outputs`\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we reconstruct the encoders and decoders using the weights of the trained model. We do the reconstruction because the model was trained as one, however it's more efficient for us to have a separate encoder and decoder. This is because during inference the encoder is only ran once to get an encoded representation (context vector) of the input however, the decoder must be ran each time an output token is generated. If we used the model in its original trained form, then the encoder would be ran unnecessarily each time a new output token was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, state_h_enc, state_c_enc = translator.layers[2].output\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the inference process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(test_input):\n",
    "  # encode the input as state vectors.\n",
    "  states_value = encoder_model.predict(test_input)\n",
    "\n",
    "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "  target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "  decoded_sentence = ''\n",
    "\n",
    "  # decoding loop\n",
    "  stop_condition = False\n",
    "  while not stop_condition:\n",
    "    # get possible output tokens (with probabilities) and states\n",
    "    output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "    # choose token with highest probability and add it to the decoded sentence\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "    decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "    # stop if you find stop token or reach max length\n",
    "    if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "      stop_condition = True\n",
    "\n",
    "    # update the target sequence and state vals which will be used to predict in the next iteration (of the loop)\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, sampled_token_index] = 1.\n",
    "    states_value = [hidden_state, cell_state]\n",
    "\n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate sample phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(20):\n",
    "  test_input = encoder_input_data[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(test_input)\n",
    "  print('-')\n",
    "  print('Input sentence:', input_docs[seq_index])\n",
    "  print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
